{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e47b12f",
   "metadata": {},
   "source": [
    "# Tutorial: Scoring Quality Tasks \n",
    "\n",
    "\n",
    "This tutorial will describe the flow for calculating scores for quality tasks and walk you through the creation of each one. Here, quality tasks are defined as any of the following (to understand more about each of these tasks, refer to the main Dataloop documentation linked):\n",
    "\n",
    "1. [Qualification tasks](https://dataloop.ai/docs/qualification-honeypot)\n",
    "2. [Honeypot tasks](https://dataloop.ai/docs/qualification-honeypot#honeypot)\n",
    "3. [Consensus tasks](https://dataloop.ai/docs/consensus)\n",
    "\n",
    "\n",
    "In general, an annotator will receive an assignment to complete their annotation task. For a given item in a consensus task, each assignment will be cross-compared with every other assignment. In the case of qualification and honeypot tasks, each item will only have one assignment associated with it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43728021-6b57-47af-98ae-b4ca70988da8",
   "metadata": {},
   "source": [
    "### _Behind the scenes_\n",
    "\n",
    "Task scores will be determined by all items in the task. To calculate an item score, the following steps will be taken:\n",
    "- for each item, all annotations will be collected by assignment\n",
    "- for qualification and honeypot tasks, all annotations *not* associated with the assignment will be considered the ground truth\n",
    "    - we assume there is only *one* assignment for qualification and honeypot tasks\n",
    "- for consensus tasks, there are no ground truth annotations because we are comparing each assignee to every other assignee\n",
    "    - during comparison, each set of assignee annotations will be compared twice, one as a reference set and once as the test set\n",
    "\n",
    "\n",
    "The flow for the functions and entities used to calculate the score are illustrated here:\n",
    "![Flow chart for scoring](../assets/scoring_flow.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d0bcff-3d37-46cf-a3c0-f5ad1aaa2945",
   "metadata": {},
   "source": [
    "## Scores created\n",
    "\n",
    "During scoring, the following scores will be created for each annotation:\n",
    "\n",
    "- `raw_annotation_scores` -  e.g. geometry, label, attribute\n",
    "- `annotation_overall` - the mean of each annotation’s raw scores\n",
    "- `user_confusion_score` - the mean of every annotation overall score, relative to ref or another assignee\n",
    "- `item_confusion_score` - the count of the number of label pairs associated with the assignee’s label, relative to the reference’s label\n",
    "- `item_overall_score` - the mean value of *each* annotation overall score associated with an item\n",
    "\n",
    "**1) Raw annotation scores:** \n",
    "\n",
    "There are three types of scores for annotations: geometry (such as IOU), label, and attribute. These scores can be determined by the user, and the default is to include all three scores, and the default value is 1 (which can be modified).\n",
    "\n",
    "**2) Annotation overall**\n",
    "\n",
    "This is the mean value for all raw annotation scores per annotation. \n",
    "\n",
    "**3) User confusion score**\n",
    "\n",
    "The value of this score represents the mean annotation score a given assignee has, relative to raw scores when comparing it to another set of annotations (either the reference or another assignee). \n",
    "\n",
    "**4) Item confusion score**\n",
    "\n",
    "The value of this score represents the count for a label annotated by a given assignee, relative to label each label class in the other set of annotations (either reference or another assignee).\n",
    "\n",
    "**5) Item overall score**\n",
    "\n",
    "This is the mean value of all annotations associated with an item, averaging the mean overall annotation score.\n",
    "\n",
    "Any calculated and uploaded scores will replace any previous scores for all items of a given task.\n",
    "\n",
    "_Note about videos_: Video scores will differ slightly from image scores. Video sores are calculated frame by frame, and then specific annotation scores will be the average of these scores across all relevant frames for that specific annotation. Confusion scores are not calculated due to the multi-frame nature of videos. Item overall scores remain an average of all annotations of the video item."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630f0748-da5c-4bc9-b799-362263957cfb",
   "metadata": {},
   "source": [
    "### Annotation types supported \n",
    "\n",
    "Scoring is currently supported for quality tasks with the following annotation types (with geometry score method in parentheses, where applicable):\n",
    "- classification\n",
    "- bounding box (IOU)\n",
    "- polygon (IOU)\n",
    "- segmentation (IOU)\n",
    "- point (distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b072366b-d0ff-4aa6-bc42-3bd093f6e602",
   "metadata": {},
   "source": [
    "### Regular vs “confusion” scores\n",
    "\n",
    "There are generally two kinds of scores: regular scores, and “confusion” scores. \n",
    "\n",
    "Regular scores show the level of agreement or overlap between two sets of annotations. They use the ID of the entities being compared for the `entityID` and `relative` fields. This can be for comparing annotations or items. `value` will typically be a number between 0 and 1. \n",
    "\n",
    "There are two types of confusion scores: item label confusion, and user confusion. **Item label confusion** shows the number of instances in which an assignee’s label corresponds with the ground truth labels. \n",
    "\n",
    "_Ground truth annotations_:\n",
    "\n",
    "![Cat v dog](../assets/cat_dog_annotations_1.png)\n",
    "\n",
    "`item = dl.items.dl(item_id='64c0fc0730b03f27ca3a58db')`\n",
    "\n",
    "_Assignee annotations_:\n",
    "\n",
    "![Cat v dog](../assets/cat_dog_annotations_2.png)\n",
    "\n",
    "`item = dl.items.dl(item_id='64c0f2e1ec9103d52eaedbe2')`\n",
    "\n",
    "\n",
    "In this example item, the ground truth has 3 for each cat and dog class. The assignee however, labels 1 as cat and 5 as dog. This would result in the following item label confusion scores:\n",
    "\n",
    "```python\n",
    "{\n",
    "        \"type\": \"label_confusion\",\n",
    "        \"value\": 1,\n",
    "        \"entityId\": \"cat\",\n",
    "        \"context\": {\n",
    "            \"relative\": \"cat\",\n",
    "            \"taskId\": \"<TASK_ID>\",\n",
    "            \"itemId\": \"<ITEM_ID\">,\n",
    "            \"datasetId\": \"<DATASET_ID>\"\n",
    "        }\n",
    "},\n",
    "{\n",
    "        \"type\": \"label_confusion\",\n",
    "        \"value\": 3,\n",
    "        \"entityId\": \"dog\",\n",
    "        \"context\": {\n",
    "            \"relative\": \"dog\",\n",
    "            \"taskId\": \"<TASK_ID>\",\n",
    "            \"itemId\": \"<ITEM_ID\">,\n",
    "            \"datasetId\": \"<DATASET_ID>\"\n",
    "        }\n",
    "},\n",
    "{\n",
    "        \"type\": \"label_confusion\",\n",
    "        \"value\": 2,\n",
    "        \"entityId\": \"dog\",\n",
    "        \"context\": {\n",
    "            \"relative\": \"cat\",\n",
    "            \"taskId\": \"<TASK_ID>\",\n",
    "            \"itemId\": \"<ITEM_ID\">,\n",
    "            \"datasetId\": \"<DATASET_ID>\"\n",
    "        }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17afd6f0-3a5a-4a97-8629-f457bb6edaae",
   "metadata": {},
   "source": [
    "### Consensus Tasks\n",
    "\n",
    "Scoring for consensus tasks differs slightly from the other two qualification tasks. Instead of comparing assignee scores to a reference set, each assignee is compared against every other assignee. There will therefore be twice as many confusion scores, with every assignee used as an `entityID` and as a `relative` ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
