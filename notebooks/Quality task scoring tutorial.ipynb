{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e47b12f",
   "metadata": {},
   "source": [
    "# Tutorial: scoring for quality tasks\n",
    "\n",
    "\n",
    "This tutorial will describe the flow for calculating scores for quality tasks and walk you through the creation of each one. Here, quality tasks are defined as any of the following (to understand more about each of these tasks, refer to the main Dataloop documentation linked):\n",
    "\n",
    "1. [Qualification tasks](https://dataloop.ai/docs/qualification-honeypot)\n",
    "2. [Honeypot tasks](https://dataloop.ai/docs/qualification-honeypot#honeypot)\n",
    "3. [Consensus tasks](https://dataloop.ai/docs/consensus)\n",
    "\n",
    "\n",
    "In general, an annotator will receive an assignment to complete their annotation task. For a given item in a consensus task, each assignment will be cross-compared with every other assignment. In the case of qualification and honeypot tasks, each item will only have one assignment associated with it. \n",
    "\n",
    "\n",
    "### _Behind the scenes_\n",
    "\n",
    "Task scores will be determined by all items in the task. To calculate an item score, the following steps will be taken:\n",
    "- for each item, all annotations will be collected by assignment\n",
    "- for qualification and honeypot tasks, all annotations *not* associated with the assignment will be considered the ground truth\n",
    "    - we assume there is only *one* assignment for qualification and honeypot tasks\n",
    "- for consensus tasks, there are no ground truth annotations because we are comparing each assignee to every other assignee\n",
    "    - during comparison, each set of assignee annotations will be compared twice, one as a reference set and once as the test set\n",
    "\n",
    "\n",
    "The flow for the functions and entities used to calculate the score are illustrated here:\n",
    "![Flow chart for scoring](scoring_flow.jpg)\n",
    "\n",
    "\n",
    "\n",
    "## Annotation scores created\n",
    "\n",
    "During scoring, these scores will be created for each annotation:\n",
    "- label score\n",
    "- geometry score\n",
    "- attribute score\n",
    "- overall annotation score\n",
    "\n",
    "\n",
    "1. __Label score__ is a score of annotation label agreement that is either 0 or 1, with the default being 1 (i.e. in the case that two annotations show there is no label, it will be considered \"agreed\"). \n",
    "2. __Geometry score__ is the annotations' geometric overlap (such as IOU), while ignoring labels. Bounding boxes with high overlap and label disagreement will be scored.\n",
    "3. __Attribute score__ is the score for attribute agreement (score of 0 or 1), with a default of NA, depending on whether attributes exist in the dataset recipe.\n",
    "4. __Overall annotation__ score is the average of all scores associated with this annotation. \n",
    "\n",
    "\n",
    "Additional scores that will be added for each item:\n",
    "\n",
    "1. __Label confusion__ is the count (per item) for a given reference classification label, how many were classified as the test set classification label (reference label is the `entityID`,  and the test label is `relative` context). For example, if 10 annotations in the reference item had the label \"cat\" and in the test set 8 annotations were matched with the label \"cat\", 2 annotation with the label \"dog\", the item's label confusion score would have the following format:\n",
    "```\n",
    "label_confusion_score = Score(type=ScoreType.LABEL_CONFUSION,\n",
    "                                   value=row['counts'],\n",
    "                                   entity_id=row['first_label'],\n",
    "                                   relative=row['second_label'])\n",
    "```\n",
    "\n",
    "2. __Item overall__ is the average of all annotation scores associated with this item (note, this is *not* an average of the overall annotation score for each annotation). \n",
    "\n",
    "\n",
    "To understand these scores, we will create a dataset and quality task to demonstrate the use of each of these functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c71107",
   "metadata": {},
   "source": [
    "## Qualification task\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76121c9",
   "metadata": {},
   "source": [
    "## Honeypot task\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a2364e",
   "metadata": {},
   "source": [
    "## Consensus task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd50f75e",
   "metadata": {},
   "source": [
    "## Snacks dataset example, confusion matrix\n",
    "\n",
    "In this example, we are using a classification dataset that has three labels: ice cream, popsicle, and pizza. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
